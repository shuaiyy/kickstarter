2018-01-18 19:27:32 [scrapy] INFO: Scrapy 1.1.1 started (bot: kickstarter)
2018-01-18 19:27:32 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'kickstarter.spiders', 'ROBOTSTXT_OBEY': True, 'LOG_LEVEL': 'INFO', 'SPIDER_MODULES': ['kickstarter.spiders'], 'BOT_NAME': 'kickstarter', 'LOG_STDOUT': True, 'LOG_FILE': 'log/KickSpider_181926.log'}
2018-01-18 19:27:32 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2018-01-18 19:27:33 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2018-01-18 19:27:33 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2018-01-18 19:27:33 [scrapy] INFO: Enabled item pipelines:
['kickstarter.pipelines.MongodbPipeline']
2018-01-18 19:27:33 [scrapy] INFO: Spider opened
2018-01-18 19:27:33 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2018-01-18 19:27:36 [scrapy] ERROR: Spider error processing <GET https://www.kickstarter.com/projects/jpoprevolution/j-pop-revolution/rewards> (referer: None)
Traceback (most recent call last):
  File "D:\ProgramData\Anaconda2\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "D:\ProgramData\Anaconda2\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "D:\ProgramData\Anaconda2\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "D:\ProgramData\Anaconda2\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\ProgramData\Anaconda2\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\mypy\kickstarter\kickstarter\kickstarter\spiders\reward_spider.py", line 21, in parse
    cp = json.loads(current_project)
  File "D:\ProgramData\Anaconda2\lib\json\__init__.py", line 339, in loads
    return _default_decoder.decode(s)
  File "D:\ProgramData\Anaconda2\lib\json\decoder.py", line 364, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "D:\ProgramData\Anaconda2\lib\json\decoder.py", line 380, in raw_decode
    obj, end = self.scan_once(s, idx)
ValueError: Expecting ',' delimiter: line 1 column 14407 (char 14406)
2018-01-18 19:29:14 [scrapy] INFO: Crawled 16 pages (at 16 pages/min), scraped 10 items (at 10 items/min)
2018-01-18 19:31:00 [scrapy] INFO: Crawled 21 pages (at 5 pages/min), scraped 12 items (at 2 items/min)
2018-01-18 19:31:00 [scrapy] ERROR: Spider error processing <GET https://www.kickstarter.com/login?then=%2Fprojects%2FPaulHartner%2F1384087152%2Frewards> (referer: None)
Traceback (most recent call last):
  File "D:\ProgramData\Anaconda2\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "D:\ProgramData\Anaconda2\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "D:\ProgramData\Anaconda2\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "D:\ProgramData\Anaconda2\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\ProgramData\Anaconda2\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\mypy\kickstarter\kickstarter\kickstarter\spiders\reward_spider.py", line 16, in parse
    current_project = response.css('script').re(r'window\.current_project.+?"(\{.+?\})";')[0]
IndexError: list index out of range
2018-01-18 19:31:00 [scrapy] ERROR: Spider error processing <GET https://www.kickstarter.com/login?then=%2Fprojects%2Fisyana%2F69489148%2Frewards> (referer: None)
Traceback (most recent call last):
  File "D:\ProgramData\Anaconda2\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "D:\ProgramData\Anaconda2\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "D:\ProgramData\Anaconda2\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "D:\ProgramData\Anaconda2\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "D:\ProgramData\Anaconda2\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "d:\mypy\kickstarter\kickstarter\kickstarter\spiders\reward_spider.py", line 16, in parse
    current_project = response.css('script').re(r'window\.current_project.+?"(\{.+?\})";')[0]
IndexError: list index out of range
2018-01-18 19:31:35 [scrapy] INFO: Crawled 28 pages (at 7 pages/min), scraped 19 items (at 7 items/min)
2018-01-18 19:32:35 [scrapy] INFO: Crawled 33 pages (at 5 pages/min), scraped 27 items (at 8 items/min)
2018-01-18 19:33:48 [scrapy] INFO: Crawled 43 pages (at 10 pages/min), scraped 37 items (at 10 items/min)
2018-01-18 19:34:37 [scrapy] INFO: Crawled 50 pages (at 7 pages/min), scraped 43 items (at 6 items/min)
2018-01-18 19:35:37 [scrapy] INFO: Crawled 60 pages (at 10 pages/min), scraped 53 items (at 10 items/min)
2018-01-18 19:40:19 [scrapy] INFO: Crawled 67 pages (at 7 pages/min), scraped 61 items (at 8 items/min)
2018-01-18 19:41:00 [scrapy] INFO: Crawled 73 pages (at 6 pages/min), scraped 67 items (at 6 items/min)
2018-01-18 19:41:40 [scrapy] INFO: Crawled 79 pages (at 6 pages/min), scraped 73 items (at 6 items/min)
2018-01-18 19:42:38 [scrapy] INFO: Crawled 89 pages (at 10 pages/min), scraped 83 items (at 10 items/min)
2018-01-18 19:43:43 [scrapy] INFO: Crawled 99 pages (at 10 pages/min), scraped 93 items (at 10 items/min)
2018-01-18 19:44:33 [scrapy] INFO: Crawled 105 pages (at 6 pages/min), scraped 99 items (at 6 items/min)
2018-01-18 19:45:41 [scrapy] INFO: Crawled 119 pages (at 14 pages/min), scraped 113 items (at 14 items/min)
2018-01-18 19:46:35 [scrapy] INFO: Crawled 127 pages (at 8 pages/min), scraped 121 items (at 8 items/min)
2018-01-18 19:47:46 [scrapy] INFO: Crawled 139 pages (at 12 pages/min), scraped 133 items (at 12 items/min)
2018-01-18 19:48:41 [scrapy] INFO: Crawled 147 pages (at 8 pages/min), scraped 141 items (at 8 items/min)
2018-01-18 19:49:38 [scrapy] INFO: Crawled 155 pages (at 8 pages/min), scraped 149 items (at 8 items/min)
2018-01-18 19:50:36 [scrapy] INFO: Crawled 168 pages (at 13 pages/min), scraped 161 items (at 12 items/min)
2018-01-18 19:51:51 [scrapy] INFO: Crawled 177 pages (at 9 pages/min), scraped 171 items (at 10 items/min)
2018-01-18 19:52:41 [scrapy] INFO: Crawled 183 pages (at 6 pages/min), scraped 177 items (at 6 items/min)
2018-01-18 19:53:48 [scrapy] INFO: Crawled 189 pages (at 6 pages/min), scraped 183 items (at 6 items/min)
2018-01-18 19:54:46 [scrapy] INFO: Crawled 195 pages (at 6 pages/min), scraped 189 items (at 6 items/min)
2018-01-18 19:55:53 [scrapy] INFO: Crawled 202 pages (at 7 pages/min), scraped 195 items (at 6 items/min)
2018-01-18 19:56:43 [scrapy] INFO: Crawled 212 pages (at 10 pages/min), scraped 205 items (at 10 items/min)
2018-01-18 19:57:40 [scrapy] INFO: Crawled 223 pages (at 11 pages/min), scraped 217 items (at 12 items/min)
2018-01-18 19:59:36 [scrapy] INFO: Crawled 231 pages (at 8 pages/min), scraped 225 items (at 8 items/min)
2018-01-18 20:00:52 [scrapy] INFO: Crawled 237 pages (at 6 pages/min), scraped 231 items (at 6 items/min)
2018-01-18 20:01:35 [scrapy] INFO: Crawled 245 pages (at 8 pages/min), scraped 239 items (at 8 items/min)
2018-01-18 20:02:42 [scrapy] INFO: Crawled 255 pages (at 10 pages/min), scraped 249 items (at 10 items/min)
2018-01-18 20:03:50 [scrapy] INFO: Crawled 265 pages (at 10 pages/min), scraped 259 items (at 10 items/min)
2018-01-18 20:04:44 [scrapy] INFO: Crawled 273 pages (at 8 pages/min), scraped 267 items (at 8 items/min)
2018-01-18 20:05:47 [scrapy] INFO: Crawled 283 pages (at 10 pages/min), scraped 277 items (at 10 items/min)
2018-01-18 20:06:42 [scrapy] INFO: Crawled 291 pages (at 8 pages/min), scraped 285 items (at 8 items/min)
2018-01-18 20:07:42 [scrapy] INFO: Crawled 301 pages (at 10 pages/min), scraped 295 items (at 10 items/min)
2018-01-18 20:17:18 [scrapy] INFO: Crawled 308 pages (at 7 pages/min), scraped 301 items (at 6 items/min)
2018-01-18 20:17:36 [scrapy] INFO: Crawled 310 pages (at 2 pages/min), scraped 304 items (at 3 items/min)
2018-01-18 20:18:45 [scrapy] INFO: Crawled 319 pages (at 9 pages/min), scraped 314 items (at 10 items/min)
2018-01-18 20:19:34 [scrapy] INFO: Crawled 327 pages (at 8 pages/min), scraped 322 items (at 8 items/min)
2018-01-18 20:20:37 [scrapy] INFO: Crawled 335 pages (at 8 pages/min), scraped 330 items (at 8 items/min)
2018-01-18 20:21:34 [scrapy] INFO: Crawled 345 pages (at 10 pages/min), scraped 339 items (at 9 items/min)
2018-01-18 20:22:44 [scrapy] INFO: Crawled 351 pages (at 6 pages/min), scraped 345 items (at 6 items/min)
2018-01-18 20:23:40 [scrapy] INFO: Crawled 353 pages (at 2 pages/min), scraped 347 items (at 2 items/min)
2018-01-18 20:24:38 [scrapy] INFO: Crawled 361 pages (at 8 pages/min), scraped 355 items (at 8 items/min)
2018-01-18 20:25:46 [scrapy] INFO: Crawled 369 pages (at 8 pages/min), scraped 363 items (at 8 items/min)
2018-01-18 20:26:33 [scrapy] INFO: Crawled 377 pages (at 8 pages/min), scraped 371 items (at 8 items/min)
2018-01-18 20:27:52 [scrapy] INFO: Crawled 385 pages (at 8 pages/min), scraped 379 items (at 8 items/min)
2018-01-18 20:28:41 [scrapy] INFO: Crawled 393 pages (at 8 pages/min), scraped 387 items (at 8 items/min)
2018-01-18 20:29:37 [scrapy] INFO: Crawled 399 pages (at 6 pages/min), scraped 393 items (at 6 items/min)
2018-01-18 20:30:34 [scrapy] INFO: Crawled 407 pages (at 8 pages/min), scraped 401 items (at 8 items/min)
2018-01-18 20:32:31 [scrapy] INFO: Crawled 417 pages (at 10 pages/min), scraped 411 items (at 10 items/min)
2018-01-18 20:32:57 [scrapy] INFO: Crawled 419 pages (at 2 pages/min), scraped 413 items (at 2 items/min)
2018-01-18 20:33:34 [scrapy] INFO: Crawled 425 pages (at 6 pages/min), scraped 419 items (at 6 items/min)
2018-01-18 20:34:41 [scrapy] INFO: Crawled 433 pages (at 8 pages/min), scraped 427 items (at 8 items/min)
2018-01-18 20:35:45 [scrapy] INFO: Crawled 441 pages (at 8 pages/min), scraped 435 items (at 8 items/min)
2018-01-18 20:37:00 [scrapy] INFO: Crawled 447 pages (at 6 pages/min), scraped 441 items (at 6 items/min)
2018-01-18 20:37:37 [scrapy] INFO: Crawled 451 pages (at 4 pages/min), scraped 445 items (at 4 items/min)
2018-01-18 20:38:59 [scrapy] INFO: Crawled 459 pages (at 8 pages/min), scraped 453 items (at 8 items/min)
2018-01-18 20:39:44 [scrapy] INFO: Crawled 465 pages (at 6 pages/min), scraped 459 items (at 6 items/min)
2018-01-18 20:40:34 [scrapy] INFO: Crawled 471 pages (at 6 pages/min), scraped 465 items (at 6 items/min)
2018-01-18 20:41:45 [scrapy] INFO: Crawled 481 pages (at 10 pages/min), scraped 475 items (at 10 items/min)
2018-01-18 20:42:34 [scrapy] INFO: Crawled 487 pages (at 6 pages/min), scraped 481 items (at 6 items/min)
2018-01-18 20:43:37 [scrapy] INFO: Crawled 497 pages (at 10 pages/min), scraped 491 items (at 10 items/min)
2018-01-18 20:44:14 [scrapy] INFO: Closing spider (finished)
2018-01-18 20:44:14 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 3,
 'downloader/request_bytes': 426326,
 'downloader/request_count': 506,
 'downloader/request_method_count/GET': 506,
 'downloader/response_bytes': 37482925,
 'downloader/response_count': 503,
 'downloader/response_status_count/200': 501,
 'downloader/response_status_count/302': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 1, 18, 12, 44, 14, 481000),
 'item_scraped_count': 497,
 'log_count/ERROR': 3,
 'log_count/INFO': 70,
 'response_received_count': 501,
 'scheduler/dequeued': 505,
 'scheduler/dequeued/memory': 505,
 'scheduler/enqueued': 505,
 'scheduler/enqueued/memory': 505,
 'spider_exceptions/IndexError': 2,
 'spider_exceptions/ValueError': 1,
 'start_time': datetime.datetime(2018, 1, 18, 11, 27, 33, 401000)}
2018-01-18 20:44:14 [scrapy] INFO: Spider closed (finished)
